import streamlit as st
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from PyPDF2 import PdfReader
import os

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF ì±—ë´‡",
    page_icon="ğŸ“š",
    layout="wide"
)

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
    <style>
    .main {
        background-color: #f5f7fa;
    }
    .stTextInput > div > div > input {
        background-color: white;
    }
    .chat-message {
        padding: 1.5rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        display: flex;
        flex-direction: column;
    }
    .user-message {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
    }
    .assistant-message {
        background-color: #f1f8e9;
        border-left: 4px solid #4caf50;
    }
    </style>
    """, unsafe_allow_html=True)

# íƒ€ì´í‹€
st.title("ğŸ“š PDF ê¸°ë°˜ AI ì±—ë´‡")
st.markdown("**test.pdf** ë¬¸ì„œì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

# API í‚¤ í™•ì¸
try:
    api_key = st.secrets["GEMINI_API_KEY"]
except:
    st.error("âš ï¸ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit secretsì— GEMINI_API_KEYë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    st.stop()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if "chain" not in st.session_state:
    st.session_state.chain = None
if "processed" not in st.session_state:
    st.session_state.processed = False

# PDF ì²˜ë¦¬ í•¨ìˆ˜
@st.cache_resource
def process_pdf():
    """PDFë¥¼ ë¡œë“œí•˜ê³  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±"""
    try:
        # PDF ì½ê¸°
        pdf_reader = PdfReader("test.pdf")
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()
        
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        chunks = text_splitter.split_text(text)
        
        # ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=api_key
        )
        vectorstore = FAISS.from_texts(chunks, embeddings)
        
        return vectorstore
    except FileNotFoundError:
        st.error("âš ï¸ test.pdf íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì´ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    except Exception as e:
        st.error(f"âš ï¸ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None

# RAG ì²´ì¸ ìƒì„±
def create_chain(vectorstore):
    """Conversational Retrieval Chain ìƒì„±"""
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash-exp",
        google_api_key=api_key,
        temperature=0.3,
        convert_system_message_to_human=True
    )
    
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        memory=memory,
        return_source_documents=True,
        verbose=False
    )
    
    return chain

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("â„¹ï¸ ì •ë³´")
    st.markdown("""
    ì´ ì±—ë´‡ì€ **test.pdf** ë¬¸ì„œì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
    
    **ê¸°ëŠ¥:**
    - ğŸ“„ PDF ë¬¸ì„œ ë¶„ì„
    - ğŸ¤– Gemini 2.5 Flash ëª¨ë¸
    - ğŸ” RAG ê¸°ë°˜ ì •í™•í•œ ë‹µë³€
    - ğŸ’¬ ëŒ€í™” ê¸°ë¡ ìœ ì§€
    """)
    
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.session_state.chain = None
        st.session_state.processed = False
        st.rerun()

# PDF ì²˜ë¦¬ (ìµœì´ˆ 1íšŒ)
if not st.session_state.processed:
    with st.spinner("ğŸ“„ PDF ë¬¸ì„œë¥¼ ë¶„ì„í•˜ëŠ” ì¤‘..."):
        vectorstore = process_pdf()
        if vectorstore:
            st.session_state.chain = create_chain(vectorstore)
            st.session_state.processed = True
            st.success("âœ… ë¬¸ì„œ ë¶„ì„ ì™„ë£Œ! ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            st.stop()

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
for message in st.session_state.messages:
    css_class = "user-message" if message["role"] == "user" else "assistant-message"
    icon = "ğŸ‘¤" if message["role"] == "user" else "ğŸ¤–"
    
    st.markdown(f"""
        <div class="chat-message {css_class}">
            <div style="font-weight: bold; margin-bottom: 0.5rem;">{icon} {message["role"].upper()}</div>
            <div>{message["content"]}</div>
        </div>
    """, unsafe_allow_html=True)

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.markdown(f"""
        <div class="chat-message user-message">
            <div style="font-weight: bold; margin-bottom: 0.5rem;">ğŸ‘¤ USER</div>
            <div>{prompt}</div>
        </div>
    """, unsafe_allow_html=True)
    
    # AI ì‘ë‹µ ìƒì„±
    with st.spinner("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            response = st.session_state.chain({"question": prompt})
            answer = response["answer"]
            
            # AI ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "assistant", "content": answer})
            
            # AI ë©”ì‹œì§€ í‘œì‹œ
            st.markdown(f"""
                <div class="chat-message assistant-message">
                    <div style="font-weight: bold; margin-bottom: 0.5rem;">ğŸ¤– ASSISTANT</div>
                    <div>{answer}</div>
                </div>
            """, unsafe_allow_html=True)
            
        except Exception as e:
            st.error(f"âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# í‘¸í„°
st.markdown("---")
st.markdown(
    "<div style='text-align: center; color: gray;'>Powered by Gemini 2.5 Flash & LangChain</div>",
    unsafe_allow_html=True
)